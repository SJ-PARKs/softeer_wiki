# 오늘 리뷰

### Crawling 코드 리뷰
- 전날에 작성한 크롤링 코드를 서로 비교했다.  
- 각자 다른 라이브러리를 사용하여 **Selenium, Playwright, API 요청 기반 스크래핑** 방식으로 구현했다.  
- 각 방식의 특성을 비교해보면서, API 요청 기반으로도 해당 페이지에서 충분히 구현이 가능하다는 점을 확인하고, 팀 내에서 API 요청 기반 방식이 가장 효율적일 것이라는 결론이 났다.


### Crawling 어떻게 할지
- 로컬이 아닌 클라우드 환경에서 crawling을 한다고 가정했을 때, 처음에 크게 3가지를 고려했다.

1. **EC2 + Airflow**
2. **Lambda + Event**
3. **ECS**

- **EC2 + Airflow**  
  - Airflow 구축 시 최소 RAM 8GB 이상이 필요하고 작업 주기가 짧지 않으며 워크플로우도 복잡하지 않아 리소스 낭비라고 판단하여 제외했다.

- **ECS**  
  - 컨테이너 기반 병렬 처리가 가능하지만 비용 측면에서 일반 Lambda보다 비쌀 것이라 판단했다.

→ 결론적으로 비용 및 운영 측면을 고려했을 때 Lambda가 최선이라고 판단하여 접근

### Lambda 사용 시 고려사항
- Lambda는 **최대 실행 시간 15분 제한**이 존재하는 것을 알게되었다.
- 전체 작업이 로컬 기준 **6시간 이상 소요**되는 상황이므로 Lambda를 사용할 경우 **함수를 여러 개로 분할**할 수밖에 없다.

이에 따라 다음과 같은 구조를 고려했다.

1. 먼저 **상품 상세페이지 URL만 수집하여 리스트 생성**
2. 생성된 **URL 리스트를 인자로 받아 Worker Lambda가 상세 정보를 병렬 수집**

### 병렬 처리 구조 검토
- 병렬 처리 구조를 위해 **SQS vs Step Functions Map State**를 비교 검토했다.
- 두 서비스 모두 아직 실제로 사용해보지는 않아 추상적으로만 이해하고 있다.
  관련 고민을 Dangle님께 여쭤보았고 친절하게 설명을 들을 수 있었다.

### 공임비 관련 조사
- 인터넷에 공개된 블루핸즈 공임비 자료를 확인한 결과, 지점별로 시간당 공임비가 상이하게 나타났다. 
- 해당 정보가 최신 기준인지 신뢰하기 어려워, 실제 현장 확인을 위해 인근 블루핸즈 3곳 방문했다.(나를 제외한 다른 팀원들이 갔다)
- 이에 따라 시간당 공임비가 약 11~13만원 정도로 수렴하고 바가지 기준을 명확히 세우는게 과제라는 것을 느꼈다.



# KPT 회고

---

## Keep

> 이번에 잘 해서 계속 유지했으면 좋다고 생각하는 행동

- 코드 리뷰 과정에서 기존에 사용하던 일반적인 crawling / scraping 라이브러리 방식이 아닌 더 좋은 방법을 발견할 수 있었다. 만약 이런 비교 과정을 거치지 않았다면 그 방법은 평생 몰랐을거 같다.
- 어떤거를 동시에 하고 각자 다른 방식으로 구현해 결과를 공유한 뒤 비교하면서 best 찾는것이 도움이 되는거같다.

## Problem

> 문제가 발생한 행동들, 개선이 필요한 부분

- 해야 할 것들을 생각했을 때 정말 시간이 얼마 남지 않았다고 느꼈다.  
- 팀원들끼리 프로젝트 목표를 다시 remind했을때, 설날 전까지 반드시 어느 정도 결과를 만들어야겠다고 생각했다.
→ 조금 더 일찍 시작해서 지금쯤 crawling을 끝냈다면 어땠을까 하는 아쉬움도 들었다.




## Try

> 다음 번에 새롭게 시도했으면 좋을 행동들
- 


## 📎 회고 메모 (선택)

- **느낀 점**: 기술적으로 논의하는게 재밌다.

- **다음 회고에서 확인할 것**: 

